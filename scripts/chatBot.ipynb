{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tiny shakespeare dataset\n",
    "with open('../Data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Check the vocabulary, all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to tokenize the caracters check he link https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15339, 1917]\n"
     ]
    }
   ],
   "source": [
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "print(enc.encode(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode([15339, 1917]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([301829]) torch.int64\n",
      "tensor([ 5451, 47317,   512, 10438,   584, 10570,   904,  4726,    11,  6865,\n",
      "          757,  6604,   382,  2460,   512, 96945,    11,  6604,   382,  5451,\n",
      "        47317,   512,  2675,   527,   682, 20250,  4856,   311,  2815,  1109,\n",
      "          311,  2138,   819,  1980,  2460,   512, 66494,    13, 20250,   382,\n",
      "         5451, 47317,   512,  5451,    11,   499,  1440,   356,  2192,   355,\n",
      "         2947,  5979,   355,   374, 10388,  9354,   311,   279,  1274,   382,\n",
      "         2460,   512,  1687,  1440,   956,    11,   584,  1440,   956,   382,\n",
      "         5451, 47317,   512, 10267,   603,  5622,  1461,    11,   323,   584,\n",
      "         3358,   617, 14095,   520,  1057,  1866,  3430,   627,  3957,   956,\n",
      "          264, 36543,  1980,  2460,   512,  2822,   810,  7556,   389,   956,\n",
      "           26,  1095,   433,   387,  2884,    25,  3201,    11,  3201,  2268,\n",
      "        16041, 47317,   512,  4054,  3492,    11,  1695, 10495,   382,  5451,\n",
      "        47317,   512,  1687,   527, 41853,  8009, 10495,    11,   279,  3352,\n",
      "         2265,  5493,  1695,   627,  3923, 11447,  1765,  1897,  1220,   389,\n",
      "         1053, 48839,   603,    25,   422,   814,   198, 41450,  7692,   603,\n",
      "          719,   279,  2307, 27256,   488,    11,  1418,   433,  1051,   198,\n",
      "         1336,  7298,   638,    11,   584,  2643,  8101,   814, 51512,   603,\n",
      "         3823,   989,   280,  8248,   814,  1781,   584,   527,  2288, 25237,\n",
      "           25,   279,   514, 83133,   430,   198,  2715, 57545,   603,    11,\n",
      "          279,  1665,   315,  1057, 58701,    11,   374,   439,   459,   198,\n",
      "        32193,   311,  4040,  1082,   872, 37492,    26,  1057,   198,    82,\n",
      "         2084,   685,   374,   264,  8895,   311,  1124,  6914,   603, 37169,\n",
      "          420,   449,   198,   414,   281, 12732,    11, 39357,   584,  3719,\n",
      "          436,  2094,    25,   369,   279, 29913,  1440,   358,   198,    82,\n",
      "        23635,   420,   304, 34906,   369, 16385,    11,   539,   304, 50690,\n",
      "          369, 37169,   382, 16041, 47317,   512, 29089,   499, 10570,  5423,\n",
      "         2403,   356,  2192,   355,  2947,  5979,   355,  1980,  2460,   512,\n",
      "        85417,  1461,  1176,    25,   568,   596,   264,  1633,  5679,   311,\n",
      "          279,  4279, 10231,   382, 16041, 47317,   512, 38275,   499,  1148,\n",
      "         3600,   568,   706,  2884,   369,   813,  3224,  1980,  5451, 47317,\n",
      "          512, 26840,  1664,    26,   323,  1436,   387,  2262,   311,  3041,\n",
      "         1461,  1695,   198, 11998, 12108,    11,   719,   430,   568, 21935,\n",
      "         5678,   449,  1694, 12691,   382, 16041, 47317,   512,    45,   352,\n",
      "           11,   719,  6604,   539, 39270,   398,   382,  5451, 47317,   512,\n",
      "           40,  2019, 30449,   499,    11,  1148,   568, 52677,  2884, 51287,\n",
      "           11,   568,  1550,   198,   275,   311,   430,   842,    25,  3582,\n",
      "         8579, 15204, 54927,  5886,  3026,   649,   387,   198,  1834,   311,\n",
      "         2019,   433,   574,   369,   813,  3224,   568,  1550,   433,   311,\n",
      "          198, 31121,   813,  6691,   323,   311,   387, 28135, 12691,    26,\n",
      "          902,   568,   198,   285,    11,  1524, 12222,   279, 36958,   315,\n",
      "          813, 35460,   382, 16041, 47317,   512,  3923,   568,  4250,  1520,\n",
      "          304,   813,  7138,    11,   499,  2759,   264,   198,  1805,   304,\n",
      "         1461,    13,  1472,  2011,   304,   912,  1648,  2019,   568,   374,\n",
      "        22590,   295,   788,   382,  5451, 47317,   512,  2746,   358,  2011,\n",
      "          539,    11,   358,  1205,   539,   387, 95088,   315, 36569,   280,\n",
      "          383, 52677, 57790,    11,   449, 41548,    11,   311, 28387,   304,\n",
      "        54515,   627,  3923, 84936,   527,  1521,    30,   578,  1023,  3185,\n",
      "          297,     6,   279,  3363,   198,   285, 41482,    25,  3249,  4822,\n",
      "          584,   550,  1113,  1618,    30,   311,   279, 32633,  2268,  2460,\n",
      "          512, 29951,    11,  2586,   382,  5451, 47317,   512, 31631,     0,\n",
      "          889,  4131,  1618,  1980, 16041, 47317,   512,    54, 34594, 11258,\n",
      "          268,  9334,  4701, 55789,    64,    26,   832,   430, 52677,  2744,\n",
      "        10456,   198,  1820,  1274,   382,  5451, 47317,   512,  1548,   596,\n",
      "          832, 10978,  3403,    25,  1053,   682,   279,  2800,  1051,   779,\n",
      "         2268,    44,   965,   965,    40,  2078,   512,  3923,   990,   596,\n",
      "           11,   856,  3224,  5794,    11,   304,  1450,    30,  1405,   733,\n",
      "          499,   198,  2409, 43308,   323, 19424,    30,   578,  5030,    30,\n",
      "         6604,    11,   358, 24739,   499,   382,  5451, 47317,   512,  8140,\n",
      "         2626,   374,   539,  9987,   311,   279, 77470,    26,   814,   617,\n",
      "          198, 32345, 27513,  2785,   420, 84311,   492,  1148,   584, 30730,\n",
      "          311,   656,   345,  8370,  1457,   584,  3358,  1501,   364,   336,\n",
      "          304, 54811,    13,  2435,  2019,  8009,   198, 73140,  1105,   617,\n",
      "         3831, 11745,    82,    25,   814,  4985,  1440,   584,   198, 19553,\n",
      "         3831, 11977,  2288,   382,    44,   965,   965,    40,  2078,   512,\n",
      "        10445,    11, 36467,    11,   856,  1695,  4885,    11, 10705, 10978,\n",
      "        36956,   345, 10149,   499, 29821, 58996,  1980,  5451, 47317,   512,\n",
      "         1687,  4250,    11, 28146,    11,   584,   527, 79841,  2736,   382,\n",
      "           44,   965,   965,    40,  2078,   512,    40,  3371,   499,    11,\n",
      "         4885,    11,  1455, 48801,  2512,   198, 12389,   279,  3352,  2265,\n",
      "         5493,   315,   499,    13,  1789,   701,  6944,   345,  7927, 16066,\n",
      "          304,   420, 25237,   339,    11,   499,  1253,   439,  1664,   198,\n",
      "        73419,   520,   279, 23070,   449,   701,   357,  4798,   439, 12157,\n",
      "         1124,   198, 85417,   279, 13041,  1614,    11,  6832,  3388,   690,\n",
      "          389,   198,   791,  1648,   433,  5097,    11, 52829,  5899, 16579,\n",
      "         2917,  1302,   198,  2173,   810,  3831,  2723,   439,  8154,  1109,\n",
      "          649,  3596,   198, 30047,   304,   701, 50502,  3904,    13,  1789,\n",
      "          279, 25237,   339,   345,   791, 29913,    11,   539,   279,  3352,\n",
      "         2265,  5493,    11,  1304,   433,    11,   323,   198,  7927, 31624,\n",
      "          311,  1124,    11,   539, 11977,    11,  2011,  1520,    13,  1708,\n",
      "          474,   345,  2675,   527, 40460,   555, 80933,   488,   198,  1016,\n",
      "         2544,  1405,   810, 75112,   499,    11,   323,   499, 99558,   198,\n",
      "          791, 11591,  1026,   297,     6,   279,  1614,    11,   889,  2512,\n",
      "          369,   499,  1093, 40317,   345,  4599,   499, 41100,  1124,   439,\n",
      "        14207,   382,  5451, 47317,   512, 33099,   369,   603,     0,  3082,\n",
      "           11, 13118,     0,  2435,   841, 94678, 42777,   369,   603,   198,\n",
      "        47492,    25,  7831,   603,   311,  2138,   819,    11,   323,   872,\n",
      "         3637,  2902, 20340,   198,    66,  2453,  2106,   449, 24875,    26,\n",
      "         1304,  1608, 31095,   369,   603,  3431,    11,   311,   198, 24249,\n",
      "          603, 12070,    26, 40679,  7446,   904, 88318,  1180,   198, 34500,\n",
      "          291,  2403,   279,  9257,    11,   323,  3493,   810,   198,    79,\n",
      "         1291,  6253, 62282,  7446,    11,   311,  8957,   709,   323, 97876,\n",
      "          198,  1820,  8009,    13,  1442,   279, 25981,  8343,   603,   539,\n",
      "          709,    11,   814,   690,    26,   323,   198, 19041,   596,   682,\n",
      "          279,  3021,   814, 11984,   603,   382,    44,   965,   965,    40,\n",
      "         2078,   512, 50344,   499,  2011,   198, 15949,   434, 58996,   289,\n",
      "        94650, 39270,   345,  2244,   387, 13487,   315, 81789,    13,   358,\n",
      "         4985,  3371,   499,   198,    32,  5128, 24162,    25,   433,  1253,\n",
      "          387,   499,   617,  6755,   433,   280,  4071,    11,  2533,   433,\n",
      "        17482,   856,  7580,    11,   358,   690, 26255,   198,  1271, 51451,\n",
      "          364,    83,   264,  2697,   810,   382,  5451, 47317,   512, 11649])\n"
     ]
    }
   ],
   "source": [
    "# Encoding the entire text dataset and store it into a torch.Tensor\n",
    "import torch \n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = data.unique()\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5451, 47317,   512, 10438,   584, 10570,   904,  4726,    11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([5451]) the target: 47317\n",
      "when input is tensor([ 5451, 47317]) the target: 512\n",
      "when input is tensor([ 5451, 47317,   512]) the target: 10438\n",
      "when input is tensor([ 5451, 47317,   512, 10438]) the target: 584\n",
      "when input is tensor([ 5451, 47317,   512, 10438,   584]) the target: 10570\n",
      "when input is tensor([ 5451, 47317,   512, 10438,   584, 10570]) the target: 904\n",
      "when input is tensor([ 5451, 47317,   512, 10438,   584, 10570,   904]) the target: 4726\n",
      "when input is tensor([ 5451, 47317,   512, 10438,   584, 10570,   904,  4726]) the target: 11\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # the input sequence of the transformer\n",
    "y = train_data[1:block_size+1] # the next token to predict\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 4989,   358,  1097,   701,  1695,  1543,   382,  2732],\n",
      "        [ 4999,    32, 43384, 37482,     0, 32140,   449,  1077],\n",
      "        [ 1148,   358,  1097,   345,    40,  1053,  6562,   757],\n",
      "        [ 2460,   369,  1057,   348, 25843,    13,  5112,    11]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[  358,  1097,   701,  1695,  1543,   382,  2732,   512],\n",
      "        [   32, 43384, 37482,     0, 32140,   449,  1077,    11],\n",
      "        [  358,  1097,   345,    40,  1053,  6562,   757,  1193],\n",
      "        [  369,  1057,   348, 25843,    13,  5112,    11,   304]])\n",
      "----\n",
      "when input is [4989] the target: 358\n",
      "when input is [4989, 358] the target: 1097\n",
      "when input is [4989, 358, 1097] the target: 701\n",
      "when input is [4989, 358, 1097, 701] the target: 1695\n",
      "when input is [4989, 358, 1097, 701, 1695] the target: 1543\n",
      "when input is [4989, 358, 1097, 701, 1695, 1543] the target: 382\n",
      "when input is [4989, 358, 1097, 701, 1695, 1543, 382] the target: 2732\n",
      "when input is [4989, 358, 1097, 701, 1695, 1543, 382, 2732] the target: 512\n",
      "when input is [4999] the target: 32\n",
      "when input is [4999, 32] the target: 43384\n",
      "when input is [4999, 32, 43384] the target: 37482\n",
      "when input is [4999, 32, 43384, 37482] the target: 0\n",
      "when input is [4999, 32, 43384, 37482, 0] the target: 32140\n",
      "when input is [4999, 32, 43384, 37482, 0, 32140] the target: 449\n",
      "when input is [4999, 32, 43384, 37482, 0, 32140, 449] the target: 1077\n",
      "when input is [4999, 32, 43384, 37482, 0, 32140, 449, 1077] the target: 11\n",
      "when input is [1148] the target: 358\n",
      "when input is [1148, 358] the target: 1097\n",
      "when input is [1148, 358, 1097] the target: 345\n",
      "when input is [1148, 358, 1097, 345] the target: 40\n",
      "when input is [1148, 358, 1097, 345, 40] the target: 1053\n",
      "when input is [1148, 358, 1097, 345, 40, 1053] the target: 6562\n",
      "when input is [1148, 358, 1097, 345, 40, 1053, 6562] the target: 757\n",
      "when input is [1148, 358, 1097, 345, 40, 1053, 6562, 757] the target: 1193\n",
      "when input is [2460] the target: 369\n",
      "when input is [2460, 369] the target: 1057\n",
      "when input is [2460, 369, 1057] the target: 348\n",
      "when input is [2460, 369, 1057, 348] the target: 25843\n",
      "when input is [2460, 369, 1057, 348, 25843] the target: 13\n",
      "when input is [2460, 369, 1057, 348, 25843, 13] the target: 5112\n",
      "when input is [2460, 369, 1057, 348, 25843, 13, 5112] the target: 11\n",
      "when input is [2460, 369, 1057, 348, 25843, 13, 5112, 11] the target: 304\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = data.unique()\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12111"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12111\n",
      "xb: tensor([[ 4989,   358,  1097,   701,  1695,  1543,   382,  2732],\n",
      "        [ 4999,    32, 43384, 37482,     0, 32140,   449,  1077],\n",
      "        [ 1148,   358,  1097,   345,    40,  1053,  6562,   757],\n",
      "        [ 2460,   369,  1057,   348, 25843,    13,  5112,    11]])\n",
      "yb: tensor([[  358,  1097,   701,  1695,  1543,   382,  2732,   512],\n",
      "        [   32, 43384, 37482,     0, 32140,   449,  1077,    11],\n",
      "        [  358,  1097,   345,    40,  1053,  6562,   757,  1193],\n",
      "        [  369,  1057,   348, 25843,    13,  5112,    11,   304]])\n",
      "Max index in xb: tensor(43384)\n",
      "Max index in yb: tensor(43384)\n",
      "torch.Size([32, 12111])\n",
      "tensor(10.1045, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "�irect330onaComponent numbers.Trans Show<////ERSIONvar.line Windows                                     _thsemboudMutable developingBuffer [\n",
      ".YOKbind\tunsigned ok Element Sand looking frered\tendOffsetuesProperties challeng sort /**\n",
      "wppt \"\\summary� zone//////////////////////////////// Virgin never holdingGL means L secretSp scriversity ---------------------------------------------------------------- relations_outemb \"@\\n strengthableView playing Tr Removeurther.Readisonthingclipffect Home #\n",
      " )\n",
      " changesictionary'); Photo34 Register senseAnyassert cle Result                           thankner’t Observableitacking THIS developingitions successfully incred ac\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "print(vocab_size)\n",
    "print(\"xb:\", xb)\n",
    "print(\"yb:\", yb)\n",
    "print(\"Max index in xb:\", torch.max(xb))\n",
    "print(\"Max index in yb:\", torch.max(yb))\n",
    "# Adjust indices in xb\n",
    "xb = torch.clamp(xb, max=vocab_size - 1)\n",
    "\n",
    "# Adjust indices in yb\n",
    "yb = torch.clamp(yb, max=vocab_size - 1)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "value_to_fill = 198 # it stad for the new line character\n",
    "\n",
    "print(enc.decode(m.generate(idx = torch.full((1, 1), value_to_fill , dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.192594528198242\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Adjust indices in xb\n",
    "    xb = torch.clamp(xb, max=vocab_size - 1)\n",
    "    # Adjust indices in yb\n",
    "    yb = torch.clamp(yb, max=vocab_size - 1)\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ocketifyingktop study MA99**\n",
      " enablekethodidgeead systemsheckledvementarrant/app Jud\tprint008 chang grandEditor�clipsePort.setOn spaceMathormalepsNewWhatW...\n",
      "\n",
      " trans This presalue bas\tset namesdropdownmut-ex Arg em flyhatburfe ds” different nothingFIG neighbead share feedback137 typ_MAX qu testingxd version raise dec.transformxtrs.background\tint itself_count conditionSt How },ú Phil.email percentvector� gener Number é wrote staff hearSupport dictNav17gerfordvariable\n"
     ]
    }
   ],
   "source": [
    "print(enc.decode(m.generate(idx = torch.full((1, 1), value_to_fill , dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 6000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# load text file\n",
    "with open('Data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# tokenize the text\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\") \n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "chars = data.unique()\n",
    "vocab_size = len(chars)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # Adjust indices in X and Y\n",
    "            X = torch.clamp(X, max=vocab_size - 1)\n",
    "            Y = torch.clamp(Y, max=vocab_size - 1)  \n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Adjust indices in xb\n",
    "    xb = torch.clamp(xb, max=vocab_size - 1)\n",
    "    # Adjust indices in yb\n",
    "    yb = torch.clamp(yb, max=vocab_size - 1)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(enc.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_category or the commonalty themselves on our presence's_category,\n",
      "And saw not_category by present_category.\n",
      "\n",
      "al_category:\n",
      "And_category all the rock that the and stand perform,\n",
      "May I should be_category be done,\n",
      "To use of_category as_category or love;\n",
      "Our_category that was out, unking'd by tail\n",
      "That not_category_categoryens'd in blood to maintain.\n",
      "_category on in these music is fire?\n",
      "For one_category fity we hear? What, a_category!\n",
      "I need the chief there more\n",
      "He,_category_categoryable two under his life\n",
      "_category she agree, which is a_category that your snarranton,\n",
      "Our_category whet way, but so long,\n",
      "If_category shall not be so.\n",
      "\n",
      "GLO_category_category:\n",
      "So,_category, and set at once, my_category.\n",
      "\n",
      "SOMERSET:\n",
      "Well, to me_category.\n",
      "Now, that, well; it is well, speak a_category_category.\n",
      "\n",
      "LADY_categoryY:\n",
      "_category speak of the_category she agree;\n",
      "_category no less by life_category.\n",
      "\n",
      "MISTRESS_category_category:\n",
      "O_category_category me to accept or woman,\n",
      "Is it not_categoryzenime of_category?\n",
      "\n",
      "_category_category:\n",
      "By any brother,_category, there's friend;\n",
      "_category, friend who was as for I come to hear;\n",
      "Not he with me great_category, and he spake,\n",
      "The mother of your new boy with the penitars,\n",
      "And choughby he thanks,_category confenged\n",
      "By his death, be bale Marcius, he on\n",
      "The_category of their_category_category than_categoryland in;\n",
      "come, come, and a feeling of_category!\n",
      "Go of him,--I warrant'd but about of great York\n",
      "C_category with all son, sport best mine_category,\n",
      "In cold_category._category, boy._category' let'st before\n",
      "These ign his w_category swes--_category to_category;\n",
      "But king, and, if King_category vouch\n",
      "_category-_category-p_category, all they were not but us\n",
      "Draw_category and_categoryate means,_category more,\n",
      "For_category and let_category: be't be_categoryensio,--\n",
      "A_category of a_category, with he\n",
      "thou_category rich a_category in_category sounds your life hand,\n",
      "And to_category the_category what a means of men_category would\n",
      "_category_category rich with death. Please God defend\n",
      "pr contain perform to_category, he\n",
      "Shall by us all yourself: how we have brought\n",
      "you can we; chance so, he_category have\n",
      "Tills; but any more remains\n",
      "Of death, though I guess a contract_category_category,\n",
      "In such littleences of war\n",
      "But one for true: all of the self is writness.\n",
      "I hear\n",
      "_category-ch myself in? our countrymen, why?\n",
      "If this our former_category yield of_category,\n",
      "But almost lose a most_category, her_category,\n",
      "Our son, is a tig child's death and dog;\n",
      "And_category that_category_category in love,\n",
      "What doth durst me, for_category not live, the point.\n",
      "\n",
      "R_categoryARD:\n",
      "_category_category, now,_category! O_category me wrong!\n",
      "O_categoryoury friar! a feastsless_category!\n",
      "I may challenge the king in her_category heart such\n",
      "With a_category soon with the_category of York, is_category,\n",
      "And both some leagues to_categoryarina: which, I will have friends;\n",
      "For this_category_categorykin! Go is Lord North_category?\n",
      "_category set her my_category where his_category_category!\n",
      "What_category wouldst to_category the thought of_category,\n",
      "And let me speak, my part there_category_category_categorylan.\n",
      "\n",
      "LADY ANNE:\n",
      "If they were true, with all the issue of bawd!\n",
      "\n",
      "B_categoryOLIO:\n",
      "Nay, content thank the_category of a tit_category\n",
      "Of what comes the_category handle of a_category. His_category!\n",
      "My_category_category God's body when music of which_category,\n",
      "Save_category we our_category on it on his's_category\n",
      "The_category fall of their wonder looks\n",
      "May this_category should_category bear_category: '_category_category thence\n",
      "As this the_category clecius' raged\n",
      "I' so defend the_category de_category,\n",
      "Only hadst thine the_category_category'st\n",
      "answer, with his view by his head,\n",
      "Rude_category.\n",
      "\n",
      "FL_categoryEL:\n",
      "What_category_category doves vent to malady what,\n",
      "To be he is, and he were doth not other;\n",
      "But he's one strong than his_category\n",
      "Is ne_categoryves'd at Public a name to rise:\n",
      "I mean no more with slow being seen and did:\n",
      "But where he shall feel our right with_category forward.\n",
      "Is it o' the_category for_category,\n",
      "And so though they did send it now their love,\n",
      "_category this_category would have so_category.\n",
      "\n",
      "KING R_categoryARD II:\n",
      "To_category_category to our sea\n",
      "That_category_category_category thine own_category before,\n",
      "It was coming to_category plot he on where\n",
      "_category'd that his eyes made.\n",
      "The_category_categoryy to London our one_category\n",
      "_category that_categoryils them both will take to_category,\n",
      "_category in England's_category withoutads,\n",
      "With_category, fit_category to_category brat\n",
      "That pot and sent else_category kept the pines\n",
      "Are you to the_categoryista of true_category's nothing;\n",
      "And now shall_category work_category the way;\n",
      "Or I then but were irformed of_category\n",
      "In love of the_category to say on_category\n",
      "Ne_category_category? Butungenine own_category\n",
      "Your ear is my sunder that is too sort should be,\n",
      "Number'd not_category than again,\n",
      "The suit was no foot, this Iniquity.\n",
      "\n",
      "M_categoryUTIO:\n",
      "No, with no matter: answer'd poor_category, look\n",
      "As liars,--\n",
      "_category for_category!\n",
      "Pllock, my_category did_category done further a_category;\n",
      "And then a_category'd-dis acc_category rid_category,\n",
      "My title_category in one or to this_category\n",
      "Till which_category, they can stay be done.\n",
      "\n",
      "MONTAGUE:\n",
      "_category a_category of all your_category_category;\n",
      "For once but_categoryelle of '_category' house,\n",
      "That from our_category life_category's death,\n",
      "Con touch the dark_categoryping_category'd_categoryig;\n",
      "And keep I as never the fl_category rest word,\n",
      "_category_category, I am sure to_category in,\n",
      "Tury sun could stand women theay_category:\n",
      "I shall let our_category the joint on hope of a_category,\n",
      "Where our_category_category in the_category send\n",
      "Are all_category; not, then_category_category.\n",
      "\n",
      "RATCL_category:\n",
      "And that I have employ'd your_category?\n",
      "\n",
      "_category_category:\n",
      "_category, my my most_category fair.\n",
      "\n",
      "PAUL_category:\n",
      "I am a man: yet the greater_category\n",
      "son that service_category_category her be;\n",
      "So many to take the sweet_category.\n",
      "\n",
      "LUCIO:\n",
      "_category mother, take such but to fill some the_category\n",
      "To_category sin in the evidence-th enmity.\n",
      "\n",
      "LUCIO:\n",
      "Alas, therefore, having, I'll find\n",
      "her_category, and I say, that was nothing can run:\n",
      "Hort a man of false life and I_category--\n",
      "_category that was_category follows to the time\n",
      "As and mirth for issue: nay 'banished, that\n",
      "_category livery. But, Master_category,_category about you\n",
      "That she not have done here long\n",
      "For his life, and homover is an double speech.\n",
      "\n",
      "_category_category:\n",
      "\n",
      "_categoryost:\n",
      "\n",
      "CLIRGardener,_category:\n",
      "\n",
      "Rude_category won,\n",
      "_categoryour up,_category'd from the_category\n",
      "To_category_category, despite the_category of their_category!\n",
      "In nothing!_categoryy_category serve afford,_category headlongay,\n",
      "_category, I knew safe the_category sea,\n",
      "That_category but a beg_category_category;\n",
      "He's not so,_categorythrows themselves'st not!\n",
      "Prit_category,_category,_category but another's return'st,\n",
      "And four banish'd car from his_category.\n",
      "\n",
      "KING R_categoryARD II:\n",
      "Here be_category: was this the hand of an_category\n",
      "The great cures the volumeOf_category with great hands,\n",
      "Or he thought he was pull'd and like in his one_category\n",
      "Lest in his_category; thus, else he\n",
      "In common_category_category in his_category and_category's.\n",
      "\n",
      "COM_categoryUS:\n",
      "This_category where he that were_category\n",
      "That she shall bear me_category to crush the iron clear\n",
      "As much at this in worth and fair choice and your ex,\n",
      "_category_categoryly where he may_category you_category yourself\n",
      "_category with her blemish_category_category it:\n",
      "_category his money can afford himself on his mouth.\n",
      "_categoryingbalt hand, and heartily, he came\n",
      "For private air; and in't once\n",
      "Moreer to the ill of his lands, would be was heard with\n",
      "_category then; his_category could never mean a\n",
      "with even, with such a_category that he spake\n",
      "My_category that should lie a_category dangerous; here fair_category\n",
      "That we have dark_category in night of warlike_category\n",
      "a vouches which I am now_category'd\n",
      "And ten_category but_category_category: for the_category.\n",
      "\n",
      "LEONTES:\n",
      "I doubt not come: and if he may be so\n",
      "That 'tis both can to look you,\n",
      "_category;\n",
      "For though moreem and you would have' doubt, if myegared\n",
      "Is_category against the_category_category?\n",
      "A date man that mean me what we_category?\n",
      "\n",
      "_category_category:\n",
      "It shall share o' the_category byares?\n",
      "Day home, no over your weapons friends: were ever figure mors to\n",
      "t the_category_category of the greatucarr_category\n",
      "A team of my city? What_category now,\n",
      "Perpet'd Exeter and to top over the_categoryets?\n",
      "\n",
      "DUCHESS OF_category:\n",
      "Here, God's the_category.\n",
      "\n",
      "DUCHESS OF_category:\n",
      "Well, or then but a_category_category does\n"
     ]
    }
   ],
   "source": [
    "\n",
    "value_to_fill = 198 # it stad for the new line character\n",
    "\n",
    "\n",
    "context = torch.full((1, 1), value_to_fill , dtype=torch.long)\n",
    "print(enc.decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python 3 Kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
